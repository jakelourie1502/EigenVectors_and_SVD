{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Effect of Principal Components Analysis on Classification Performance of MNIST\n",
    "\n",
    "In this notebook, we use a simple Torch model to classify MNIST digits. We then apply principle components analysis to see how well the model can classify the reduced dimension feature representations.\n",
    "\n",
    "We are going to reduce the dimensionality of our dataset(currently 28 x 28 x 1) by using eigenvectors. This will allow us to capture the most important variations between images. We will then see if we can apply just this reduced number of dimensions to a neural net and gain a similar performance level to that which we achieved with a ResNet on the full images (~85%)\n",
    "\n",
    "Steps: \n",
    "\n",
    "1- Flatten arrays \n",
    "\n",
    "2- normalize data across each dimension\n",
    "\n",
    "3- Compute N eigenvectors and the covariance matrix\n",
    "\n",
    "\n",
    "4- Project images onto eigenspace using projection matrix:  \n",
    "\n",
    "- projection = B @ (B.T@B)^-1 @ B.T } \n",
    "\n",
    "where B is new basis. B.T@B will be the identity matrix if the eigenvectors are normalised, so it would just become:\n",
    "\n",
    "- projection = B @ B.T\n",
    "\n",
    "We will experiment with various choices of N, but 50 is a good place to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Section 1: Load the data and create dataloaders to process batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat = x.reshape(x.shape[0],-1)\n",
    "x_flat = (x_flat/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data into a data object and then create the batches\n",
    "def create_dataloaders(x,y,batch_size):\n",
    "    tensor_x = torch.Tensor(x) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(y)\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "    train_data, val_data = torch.utils.data.random_split(my_dataset, [int(0.8*len(y)), len(y)-int(0.8*len(y))])\n",
    "    train_dataloader = DataLoader(train_data,batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_data,batch_size=batch_size) \n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Section 2 - Creating a model and training structure\n",
    "\n",
    "We'll create the a residuals model using torch nn module; Then create train and validation systems using Adam optimizer and CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sequential model\n",
    "class seven_layer_model(torch.nn.Module):\n",
    "    \"\"\"don't think we need to do super() because we aren't doing a separate init\"\"\"\n",
    "    def __init__(self,inputdim=50):\n",
    "        super().__init__()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.batch1 = torch.nn.BatchNorm1d(100)\n",
    "        self.batch2 = torch.nn.BatchNorm1d(100)\n",
    "        self.batch3 = torch.nn.BatchNorm1d(100)\n",
    "        self.batch4 = torch.nn.BatchNorm1d(100)\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(inputdim,100)\n",
    "        self.layer2 = torch.nn.Linear(100,100)\n",
    "        self.layer3 = torch.nn.Linear(100,100)\n",
    "        self.layer4 = torch.nn.Linear(100,100)\n",
    "        \n",
    "        self.layer5 = torch.nn.Linear(100,10)        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        for layer in self.parameters():\n",
    "            if layer == torch.nn.Linear:\n",
    "                torch.nn.init.xavier_uniform(layer.weight)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.batch1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x2 = self.tanh(x)\n",
    "        x2 = self.batch2(x)\n",
    "        x3 = self.batch3(self.tanh(self.layer3(x)))\n",
    "        x4 = self.batch4(x2 + self.tanh(self.layer4(x)))\n",
    "        x5 = self.layer5(x4)\n",
    "        output = self.softmax(x5)\n",
    "        return output\n",
    "\n",
    "\n",
    "#create forward pass and backward pass\n",
    "def train_step(model, dataloader, criterion,optimizer, accuracy_metric, printy=True):\n",
    "    model.train()\n",
    "    for x,y in dataloader:\n",
    "        outputs = model(x)\n",
    "        y = y.long()\n",
    "        loss = criterion(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = torch.argmax(outputs,axis=-1)\n",
    "        accuracy_metric(outputs,y)\n",
    "\n",
    "    acc = accuracy_metric.compute()\n",
    "\n",
    "    if printy:\n",
    "        print('Train Accuracy: ', acc)\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "def val_step(model, dataloader,accuracy_metric):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            outputs = model(x)\n",
    "            y = y.long()\n",
    "            predictions = torch.argmax(outputs,axis=-1)\n",
    "\n",
    "            accuracy_metric(outputs,y)\n",
    "\n",
    "        acc = accuracy_metric.compute()\n",
    "    print('Val Accuracy: ', acc)\n",
    "    accuracy_metric.reset()\n",
    "def train_model(model,train_dataloader,val_dataloader, criterion, optimizer, accuracy_metric, epochs,progress=True):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if progress==True and epoch in np.geomspace(1,41,10).astype('int'):\n",
    "            print(f'epoch: {epoch}')\n",
    "            train_step(model,train_dataloader, criterion, optimizer, accuracy_metric)\n",
    "            val_step(model, val_dataloader,accuracy_metric)\n",
    "        else:\n",
    "            train_step(model,train_dataloader,criterion, optimizer, accuracy_metric,printy=False)\n",
    "    print(f'End of Model results after {epoch} epochs:')\n",
    "    train_step(model,train_dataloader, criterion, optimizer, accuracy_metric)    \n",
    "    val_step(model, val_dataloader,accuracy_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = create_dataloaders(x_flat, y, 200)\n",
    "#define metrics.\n",
    "accuracy_metric = torchmetrics.Accuracy()\n",
    "\n",
    "pre_eig_model = seven_layer_model(inputdim=x_flat.shape[1])\n",
    "\n",
    "#create optimizer - which includes the model as a parameter\n",
    "optimizer = torch.optim.Adam(pre_eig_model.parameters())\n",
    "\n",
    "#create criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Train Accuracy:  tensor(0.9967)\n",
      "Val Accuracy:  tensor(0.9755)\n",
      "epoch: 2\n",
      "Train Accuracy:  tensor(0.9967)\n",
      "Val Accuracy:  tensor(0.9762)\n",
      "epoch: 3\n",
      "Train Accuracy:  tensor(0.9965)\n",
      "Val Accuracy:  tensor(0.9753)\n",
      "epoch: 5\n",
      "Train Accuracy:  tensor(0.9967)\n",
      "Val Accuracy:  tensor(0.9759)\n",
      "epoch: 7\n",
      "Train Accuracy:  tensor(0.9969)\n",
      "Val Accuracy:  tensor(0.9752)\n",
      "epoch: 11\n",
      "Train Accuracy:  tensor(0.9967)\n",
      "Val Accuracy:  tensor(0.9776)\n",
      "epoch: 17\n",
      "Train Accuracy:  tensor(0.9956)\n",
      "Val Accuracy:  tensor(0.9759)\n",
      "epoch: 27\n",
      "Train Accuracy:  tensor(0.9966)\n",
      "Val Accuracy:  tensor(0.9753)\n",
      "epoch: 40\n",
      "Train Accuracy:  tensor(0.9970)\n",
      "Val Accuracy:  tensor(0.9761)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9970)\n",
      "Val Accuracy:  tensor(0.9763)\n",
      "This took 85 second to train 40 epochs\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "train_model(pre_eig_model, train_dataloader, val_dataloader, criterion, optimizer, accuracy_metric, epochs=41)\n",
    "print(f'This took {int(time.time()-time_)} second to train 40 epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Section 3 - projecting the original data onto a lower dimensions subspace\n",
    "\n",
    "OK, so we are achieving 98% accuracy with a simple 5 layer neural network with batch norm.\n",
    "\n",
    "Now let's perform eigendecomposition and see if a reduced set of features can give us the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eigs(x):\n",
    "    #normalize data\n",
    "    def normalize_data(x):\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        mean, var = np.mean(x,axis=0), np.var(x,axis=0)\n",
    "        x = (x-mean)/np.sqrt(var+1e-7)\n",
    "        return x\n",
    "    \n",
    "    x = normalize_data(x)\n",
    "\n",
    "    def create_eigenvectors(x):\n",
    "        cov_matrix = np.cov(x,rowvar=False)\n",
    "        eig_val, eig_vec = np.linalg.eig(cov_matrix)\n",
    "        return eig_val, eig_vec, cov_matrix\n",
    "       \n",
    "    eig_val,eig_vec, cov_matrix = create_eigenvectors(x)\n",
    "    return eig_val, eig_vec, cov_matrix\n",
    "\n",
    "def project_X(eig_vec,x,eigs):\n",
    "    prin_eig_vec, prin_eig_val = eig_vec[:,:eigs], eig_val[:eigs]\n",
    "    P = prin_eig_vec @ prin_eig_vec.T\n",
    "\n",
    "    #project onto all prev_dims\n",
    "    projection_prev_dims = (P @ x.T).T\n",
    "    #project in terms of new dims\n",
    "    projection_eig_dims = x @ prin_eig_vec\n",
    "    return projection_eig_dims, projection_prev_dims, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vec, cov_matrix = create_eigs(x_flat)\n",
    "projection_eig_dims, projection_prev_dims, P = project_X(eig_vec, x_flat, 50)\n",
    "projection_eig_dims = projection_eig_dims / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection matrix is of shape: (784, 784)\n",
      "Dimensions of projected values on old space: (60000, 784)\n",
      "projection in terms of eigenbases: (60000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(f'Projection matrix is of shape: {P.shape}\\nDimensions of projected values on old space: {projection_prev_dims.shape}\\nprojection in terms of eigenbases: {projection_eig_dims.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Viewing the projected images\n",
    "\n",
    "What does a projection 'look' like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = 100\n",
    "fig, axlist = plt.subplots(nrows=1,ncols=2)\n",
    "axlist[0].imshow(x_train_part[example],cmap='gray'); \n",
    "axlist[1].imshow(1*projection_prev_dims[example].astype(float).reshape(28,28),cmap='gray'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "The eigenbases also don't look like much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcb77ee1400>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scaled up eigenvectors\n",
    "plt.imshow(100*prin_eig_vec[:,5].astype(float).reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Running the same model with lower dimensional dataset\n",
    "\n",
    "So now let's create a dataset which is the projection of the X values into the top 50 eigenbases (note, all the relevant info is contained in the array with just 50 columns), and see how this performs in our dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "train_data_eig, val_data_eig = create_dataloaders(projection_eig_dims, y,batch_size=200)\n",
    "#create model with new inputdim\n",
    "eigenmodel_50 = seven_layer_model(inputdim=projection_eig_dims.shape[1])\n",
    "\n",
    "#create optimizer linked to this model\n",
    "optimizer_eig = torch.optim.Adam(eigenmodel_50.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9951)\n",
      "Val Accuracy:  tensor(0.9776)\n",
      "This took 69 second to train 40 epochs\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "train_model(eigenmodel_50,train_data_eig, val_data_eig, criterion, optimizer_eig, accuracy_metric, epochs=41,progress=False)\n",
    "print(f'This took {int(time.time()-time_)} second to train 40 epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Testing different number of eigenvectors\n",
    "\n",
    "We aren't able to achieve quite as strong performance when we have only 50 dimensions. Let's see what happens if we try more / less eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying 1 eigenbases:\n",
      "x has shape (60000, 1)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.3044)\n",
      "Val Accuracy:  tensor(0.3080)\n",
      "This took 41 second to train\n",
      "\n",
      "Trying 2 eigenbases:\n",
      "x has shape (60000, 2)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.3758)\n",
      "Val Accuracy:  tensor(0.3732)\n",
      "This took 43 second to train\n",
      "\n",
      "Trying 5 eigenbases:\n",
      "x has shape (60000, 5)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.7737)\n",
      "Val Accuracy:  tensor(0.7584)\n",
      "This took 41 second to train\n",
      "\n",
      "Trying 10 eigenbases:\n",
      "x has shape (60000, 10)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9484)\n",
      "Val Accuracy:  tensor(0.9311)\n",
      "This took 36 second to train\n",
      "\n",
      "Trying 25 eigenbases:\n",
      "x has shape (60000, 25)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9851)\n",
      "Val Accuracy:  tensor(0.9673)\n",
      "This took 41 second to train\n",
      "\n",
      "Trying 50 eigenbases:\n",
      "x has shape (60000, 50)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9913)\n",
      "Val Accuracy:  tensor(0.9749)\n",
      "This took 40 second to train\n",
      "\n",
      "Trying 100 eigenbases:\n",
      "x has shape (60000, 100)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9934)\n",
      "Val Accuracy:  tensor(0.9753)\n",
      "This took 41 second to train\n",
      "\n",
      "Trying 200 eigenbases:\n",
      "x has shape (60000, 200)\n",
      "End of Model results after 40 epochs:\n",
      "Train Accuracy:  tensor(0.9938)\n",
      "Val Accuracy:  tensor(0.9749)\n",
      "This took 41 second to train\n"
     ]
    }
   ],
   "source": [
    "list_of_eig_n_to_try = [1,2,5,10,25,50,100,200]\n",
    "eig_val, eig_vec, cov_matrix = create_eigs(x_flat)\n",
    "\n",
    "for n in list_of_eig_n_to_try:\n",
    "    print(f'\\nTrying {n} eigenbases:')\n",
    "    \n",
    "    projection_eig_dims, projection_prev_dims, P = project_X(eig_vec, x_flat, n)\n",
    "    projection_prev_dims = projection_prev_dims / 255\n",
    "    print(f'x has shape {projection_eig_dims.shape}')\n",
    "    \n",
    "    \n",
    "    #dataset\n",
    "    train_data_eig, val_data_eig = create_dataloaders(projection_eig_dims, y,batch_size=1000)\n",
    "    \n",
    "    #create model with new inputdim\n",
    "    eigenmodel_n = seven_layer_model(inputdim=projection_eig_dims.shape[1])\n",
    "\n",
    "    #acc\n",
    "    accuracy_metric = torchmetrics.Accuracy()\n",
    "    #create optimizer linked to this model\n",
    "    optimizer_eig = torch.optim.Adam(eigenmodel_n.parameters())\n",
    "    time_ = time.time()\n",
    "    train_model(eigenmodel_n,train_data_eig, val_data_eig, criterion, optimizer_eig, accuracy_metric, epochs=41,progress=False)\n",
    "    print(f'This took {int(time.time()-time_)} second to train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "It's clear that we can achieve the same classification performance from around 50 dimensions. The time to train is slightly unstable, but it appears we can also reduce the training time of the model by around 50% using a lower dimensional representation of the data too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
